{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from featuretools.primitives import AggregationPrimitive, make_agg_primitive\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import featuretools as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_B_meta = pd.read_csv('/Users/JoonH/basic_data_B_meta.csv')\n",
    "basic_C_meta = pd.read_csv('/Users/JoonH/basic_data_C_meta.csv')\n",
    "basic_test_meta = pd.read_csv('/Users/JoonH/basic_data_test_meta.csv')\n",
    "\n",
    "eng_B_meta = pd.read_csv('/Users/JoonH/eng_data_B_meta.csv')\n",
    "eng_C_meta = pd.read_csv('/Users/JoonH/eng_data_C_meta.csv')\n",
    "eng_test_meta = pd.read_csv('/Users/JoonH/eng_data_test_meta.csv')\n",
    "\n",
    "xgb_B_meta = pd.read_csv('/Users/JoonH/eng_data_B_meta_xgb.csv')\n",
    "xgb_C_meta = pd.read_csv('/Users/JoonH/eng_data_C_meta_xgb.csv')\n",
    "xgb_test_meta = pd.read_csv('/Users/JoonH/eng_data_test_meta_xgb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_B_meta = pd.read_csv('/Users/JoonH/eng_data_gbm_B.csv')\n",
    "gbm_C_meta = pd.read_csv('/Users/JoonH/eng_data_gbm_C.csv')\n",
    "gbm_test_meta = pd.read_csv('/Users/JoonH/eng_data_gbm_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(129154, 2) (55353, 2) (48744, 2)\n"
     ]
    }
   ],
   "source": [
    "print(gbm_B_meta.shape, gbm_C_meta.shape, gbm_test_meta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_spec = pd.read_csv('total_feature_matrix_spec.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = feature_spec[feature_spec['set'] == 'train']\n",
    "test = feature_spec[feature_spec['set'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.get_dummies(train)\n",
    "test = pd.get_dummies(test)\n",
    "train_labels= train['TARGET']\n",
    "train, test = train.align(test, join = 'inner', axis = 1)\n",
    "train['TARGET'] = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devide train data into 3: train A, B, and C\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_A, train_B = train_test_split(train, test_size = 0.6, random_state = 3)\n",
    "train_B, train_C = train_test_split(train_B, test_size = 0.3, random_state = 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(123004, 1157) (129154, 1157) (55353, 1157) (48744, 1157)\n"
     ]
    }
   ],
   "source": [
    "print(train_A.shape, train_B.shape, train_C.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train labels for b_meta and c_meta\n",
    "b_meta_labels = train_B['TARGET']\n",
    "c_meta_labels = train_C['TARGET']\n",
    "del train_A, train_B, train_C, feature_spec, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ada_basic</th>\n",
       "      <th>bag_basic</th>\n",
       "      <th>gbc_basic</th>\n",
       "      <th>rf_basic</th>\n",
       "      <th>rf_2_basic</th>\n",
       "      <th>log_basic</th>\n",
       "      <th>rid_basic</th>\n",
       "      <th>sgd_basic</th>\n",
       "      <th>gnb_basic</th>\n",
       "      <th>knn_basic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ada_basic  bag_basic  gbc_basic  rf_basic  rf_2_basic  \\\n",
       "0           0          0          1          0         0           0   \n",
       "1           1          0          1          0         0           0   \n",
       "2           2          0          0          0         0           0   \n",
       "3           3          0          1          0         0           0   \n",
       "4           4          0          1          0         0           0   \n",
       "\n",
       "   log_basic  rid_basic  sgd_basic  gnb_basic  knn_basic  \n",
       "0          0          0          1          0          0  \n",
       "1          0          0          1          0          0  \n",
       "2          0          0          1          0          0  \n",
       "3          0          0          1          0          0  \n",
       "4          0          0          1          0          0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_B_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ada_eng</th>\n",
       "      <th>gbc_eng</th>\n",
       "      <th>rf_1_eng</th>\n",
       "      <th>rf_2_eng</th>\n",
       "      <th>log_eng</th>\n",
       "      <th>knn_eng</th>\n",
       "      <th>svc_eng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ada_eng  gbc_eng  rf_1_eng  rf_2_eng  log_eng  knn_eng  svc_eng\n",
       "0           0      0.0      1.0       0.0       1.0      0.0      0.0      0.0\n",
       "1           1      0.0      1.0       0.0       1.0      0.0      0.0      0.0\n",
       "2           2      0.0      1.0       1.0       1.0      0.0      0.0      0.0\n",
       "3           3      0.0      1.0       1.0       1.0      0.0      0.0      0.0\n",
       "4           4      0.0      1.0       1.0       1.0      0.0      1.0      0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_B_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>eng_xgb_1</th>\n",
       "      <th>eng_xgb_2</th>\n",
       "      <th>eng_xgb_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.219005</td>\n",
       "      <td>0.448055</td>\n",
       "      <td>0.461210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.221092</td>\n",
       "      <td>0.456430</td>\n",
       "      <td>0.466419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.196986</td>\n",
       "      <td>0.445541</td>\n",
       "      <td>0.460764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.264329</td>\n",
       "      <td>0.460987</td>\n",
       "      <td>0.470842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.245918</td>\n",
       "      <td>0.447953</td>\n",
       "      <td>0.461444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  eng_xgb_1  eng_xgb_2  eng_xgb_3\n",
       "0           0   0.219005   0.448055   0.461210\n",
       "1           1   0.221092   0.456430   0.466419\n",
       "2           2   0.196986   0.445541   0.460764\n",
       "3           3   0.264329   0.460987   0.470842\n",
       "4           4   0.245918   0.447953   0.461444"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_B_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>eng_gbm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.145794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.464633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.130807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.597577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.513868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   eng_gbm\n",
       "0           0  0.145794\n",
       "1           1  0.464633\n",
       "2           2  0.130807\n",
       "3           3  0.597577\n",
       "4           4  0.513868"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm_B_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_B_meta = gbm_B_meta.drop('Unnamed: 0',axis = 1)\n",
    "gbm_C_meta = gbm_C_meta.drop('Unnamed: 0',axis = 1)\n",
    "gbm_test_meta = gbm_test_meta.drop('Unnamed: 0',axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we obeserve that our dataframes not unified in terms of class/proba predictions and dtype. Since only xgb predicted probability, we will convert all data to hold int binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng_xgb_1</th>\n",
       "      <th>eng_xgb_2</th>\n",
       "      <th>eng_xgb_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eng_xgb_1  eng_xgb_2  eng_xgb_3\n",
       "0        0.0        0.0        0.0\n",
       "1        0.0        0.0        0.0\n",
       "2        0.0        0.0        0.0\n",
       "3        0.0        0.0        0.0\n",
       "4        0.0        0.0        0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_proba(data, name):\n",
    "    with np.nditer(data, op_flags = ['readwrite']) as it:\n",
    "        for i in it:\n",
    "            if i[...] < 0.5:\n",
    "                i[...] = 0\n",
    "            else:\n",
    "                i[...] = 1\n",
    "        data = pd.DataFrame(data, columns = [name])\n",
    "    return data\n",
    "xgb_B_meta_1 = np.array(xgb_B_meta['eng_xgb_1'])\n",
    "xgb_B_meta_2 = np.array(xgb_B_meta['eng_xgb_2'])\n",
    "xgb_B_meta_3 = np.array(xgb_B_meta['eng_xgb_3'])\n",
    "gbm_B_meta = np.array(gbm_B_meta)\n",
    "\n",
    "xgb_B_meta_1 = convert_proba(xgb_B_meta_1, 'eng_xgb_1')\n",
    "xgb_B_meta_2 = convert_proba(xgb_B_meta_2, 'eng_xgb_2')\n",
    "xgb_B_meta_3 = convert_proba(xgb_B_meta_3, 'eng_xgb_3')\n",
    "gbm_B_meta = convert_proba(gbm_B_meta, 'eng_gbm')\n",
    "\n",
    "xgb_B_meta = pd.concat([xgb_B_meta_1, xgb_B_meta_2, xgb_B_meta_3], axis = 1)\n",
    "gbm_B_meta = pd.DataFrame(gbm_B_meta, columns = ['eng_gbm'])\n",
    "xgb_B_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# worked correctly, let's try this for C and test meta data\n",
    "xgb_C_meta_1 = np.array(xgb_C_meta['eng_xgb_1'])\n",
    "xgb_C_meta_2 = np.array(xgb_C_meta['eng_xgb_2'])\n",
    "xgb_C_meta_3 = np.array(xgb_C_meta['eng_xgb_3'])\n",
    "gbm_C_meta = np.array(gbm_C_meta)\n",
    "\n",
    "xgb_C_meta_1 = convert_proba(xgb_C_meta_1, 'eng_xgb_1')\n",
    "xgb_C_meta_2 = convert_proba(xgb_C_meta_2, 'eng_xgb_2')\n",
    "xgb_C_meta_3 = convert_proba(xgb_C_meta_3, 'eng_xgb_3')\n",
    "gbm_C_meta = convert_proba(gbm_C_meta, 'eng_gbm')\n",
    "\n",
    "gbm_C_meta = pd.DataFrame(gbm_C_meta, columns = ['eng_gbm'])\n",
    "xgb_C_meta = pd.concat([xgb_C_meta_1, xgb_C_meta_2, xgb_C_meta_3], axis = 1)\n",
    "\n",
    "xgb_test_meta_1 = np.array(xgb_test_meta['eng_xgb_1'])\n",
    "xgb_test_meta_2 = np.array(xgb_test_meta['eng_xgb_2'])\n",
    "xgb_test_meta_3 = np.array(xgb_test_meta['eng_xgb_3'])\n",
    "gbm_test_meta = np.array(gbm_test_meta)\n",
    "\n",
    "xgb_test_meta_1 = convert_proba(xgb_test_meta_1, 'eng_xgb_1')\n",
    "xgb_test_meta_2 = convert_proba(xgb_test_meta_2, 'eng_xgb_2')\n",
    "xgb_test_meta_3 = convert_proba(xgb_test_meta_3, 'eng_xgb_3')\n",
    "gbm_test_meta = convert_proba(gbm_test_meta, 'eng_gbm')\n",
    "\n",
    "xgb_test_meta = pd.concat([xgb_test_meta_1, xgb_test_meta_2, xgb_test_meta_3], axis = 1)\n",
    "gbm_test_meta = pd.DataFrame(gbm_test_meta, columns = ['eng_gbm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put the train data alltogether, by B_meta, C_meta and test_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_meta = pd.concat([basic_B_meta.drop('Unnamed: 0', axis = 1), \n",
    "                    eng_B_meta.drop('Unnamed: 0', axis = 1),\n",
    "                    xgb_B_meta, gbm_B_meta], axis = 1)\n",
    "\n",
    "C_meta = pd.concat([basic_C_meta.drop('Unnamed: 0', axis = 1), \n",
    "                    eng_C_meta.drop('Unnamed: 0', axis = 1),\n",
    "                    xgb_C_meta, gbm_C_meta], axis = 1) \n",
    "\n",
    "test_meta = pd.concat([basic_test_meta.drop('Unnamed: 0', axis = 1), \n",
    "                    eng_test_meta.drop('Unnamed: 0', axis = 1),\n",
    "                    xgb_test_meta, gbm_test_meta], axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ada_basic</th>\n",
       "      <th>bag_basic</th>\n",
       "      <th>gbc_basic</th>\n",
       "      <th>rf_basic</th>\n",
       "      <th>rf_2_basic</th>\n",
       "      <th>log_basic</th>\n",
       "      <th>rid_basic</th>\n",
       "      <th>sgd_basic</th>\n",
       "      <th>gnb_basic</th>\n",
       "      <th>knn_basic</th>\n",
       "      <th>...</th>\n",
       "      <th>gbc_eng</th>\n",
       "      <th>rf_1_eng</th>\n",
       "      <th>rf_2_eng</th>\n",
       "      <th>log_eng</th>\n",
       "      <th>knn_eng</th>\n",
       "      <th>svc_eng</th>\n",
       "      <th>eng_xgb_1</th>\n",
       "      <th>eng_xgb_2</th>\n",
       "      <th>eng_xgb_3</th>\n",
       "      <th>eng_gbm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ada_basic  bag_basic  gbc_basic  rf_basic  rf_2_basic  log_basic  \\\n",
       "0          0          1          0         0           0          0   \n",
       "1          0          1          0         0           0          0   \n",
       "2          0          0          0         0           0          0   \n",
       "3          0          1          0         0           0          0   \n",
       "4          0          1          0         0           0          0   \n",
       "\n",
       "   rid_basic  sgd_basic  gnb_basic  knn_basic   ...     gbc_eng  rf_1_eng  \\\n",
       "0          0          1          0          0   ...         1.0       0.0   \n",
       "1          0          1          0          0   ...         1.0       0.0   \n",
       "2          0          1          0          0   ...         1.0       1.0   \n",
       "3          0          1          0          0   ...         1.0       1.0   \n",
       "4          0          1          0          0   ...         1.0       1.0   \n",
       "\n",
       "   rf_2_eng  log_eng  knn_eng  svc_eng  eng_xgb_1  eng_xgb_2  eng_xgb_3  \\\n",
       "0       1.0      0.0      0.0      0.0        0.0        0.0        0.0   \n",
       "1       1.0      0.0      0.0      0.0        0.0        0.0        0.0   \n",
       "2       1.0      0.0      0.0      0.0        0.0        0.0        0.0   \n",
       "3       1.0      0.0      0.0      0.0        0.0        0.0        0.0   \n",
       "4       1.0      0.0      1.0      0.0        0.0        0.0        0.0   \n",
       "\n",
       "   eng_gbm  \n",
       "0      0.0  \n",
       "1      0.0  \n",
       "2      0.0  \n",
       "3      1.0  \n",
       "4      1.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ada_basic</th>\n",
       "      <th>bag_basic</th>\n",
       "      <th>gbc_basic</th>\n",
       "      <th>rf_basic</th>\n",
       "      <th>rf_2_basic</th>\n",
       "      <th>log_basic</th>\n",
       "      <th>rid_basic</th>\n",
       "      <th>sgd_basic</th>\n",
       "      <th>gnb_basic</th>\n",
       "      <th>knn_basic</th>\n",
       "      <th>...</th>\n",
       "      <th>gbc_eng</th>\n",
       "      <th>rf_1_eng</th>\n",
       "      <th>rf_2_eng</th>\n",
       "      <th>log_eng</th>\n",
       "      <th>knn_eng</th>\n",
       "      <th>svc_eng</th>\n",
       "      <th>eng_xgb_1</th>\n",
       "      <th>eng_xgb_2</th>\n",
       "      <th>eng_xgb_3</th>\n",
       "      <th>eng_gbm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ada_basic  bag_basic  gbc_basic  rf_basic  rf_2_basic  log_basic  \\\n",
       "0          0          1          0         0           0          0   \n",
       "1          0          1          0         0           0          0   \n",
       "2          0          1          0         0           0          0   \n",
       "3          0          1          0         0           0          0   \n",
       "4          0          1          0         0           0          0   \n",
       "\n",
       "   rid_basic  sgd_basic  gnb_basic  knn_basic   ...     gbc_eng  rf_1_eng  \\\n",
       "0          0          1          0          0   ...         1.0       0.0   \n",
       "1          0          1          0          0   ...         1.0       0.0   \n",
       "2          0          1          0          0   ...         1.0       0.0   \n",
       "3          0          1          0          0   ...         1.0       0.0   \n",
       "4          0          1          0          0   ...         1.0       0.0   \n",
       "\n",
       "   rf_2_eng  log_eng  knn_eng  svc_eng  eng_xgb_1  eng_xgb_2  eng_xgb_3  \\\n",
       "0       1.0      0.0      0.0      0.0        0.0        0.0        0.0   \n",
       "1       1.0      0.0      0.0      0.0        0.0        0.0        0.0   \n",
       "2       1.0      0.0      0.0      0.0        0.0        0.0        0.0   \n",
       "3       1.0      0.0      0.0      0.0        0.0        1.0        1.0   \n",
       "4       1.0      0.0      0.0      0.0        0.0        0.0        0.0   \n",
       "\n",
       "   eng_gbm  \n",
       "0      0.0  \n",
       "1      0.0  \n",
       "2      0.0  \n",
       "3      1.0  \n",
       "4      0.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ada_basic</th>\n",
       "      <th>bag_basic</th>\n",
       "      <th>gbc_basic</th>\n",
       "      <th>rf_basic</th>\n",
       "      <th>rf_2_basic</th>\n",
       "      <th>log_basic</th>\n",
       "      <th>rid_basic</th>\n",
       "      <th>sgd_basic</th>\n",
       "      <th>gnb_basic</th>\n",
       "      <th>knn_basic</th>\n",
       "      <th>...</th>\n",
       "      <th>gbc_eng</th>\n",
       "      <th>rf_1_eng</th>\n",
       "      <th>rf_2_eng</th>\n",
       "      <th>log_eng</th>\n",
       "      <th>knn_eng</th>\n",
       "      <th>svc_eng</th>\n",
       "      <th>eng_xgb_1</th>\n",
       "      <th>eng_xgb_2</th>\n",
       "      <th>eng_xgb_3</th>\n",
       "      <th>eng_gbm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ada_basic  bag_basic  gbc_basic  rf_basic  rf_2_basic  log_basic  \\\n",
       "0          0          0          0         0           0          0   \n",
       "1          0          1          0         0           0          0   \n",
       "2          0          1          0         0           0          0   \n",
       "3          0          1          0         0           0          0   \n",
       "4          0          1          0         0           0          0   \n",
       "\n",
       "   rid_basic  sgd_basic  gnb_basic  knn_basic   ...     gbc_eng  rf_1_eng  \\\n",
       "0          0          1          0          0   ...         1.0       1.0   \n",
       "1          0          1          0          0   ...         1.0       1.0   \n",
       "2          0          1          0          0   ...         1.0       1.0   \n",
       "3          0          1          0          0   ...         1.0       1.0   \n",
       "4          0          1          0          0   ...         1.0       1.0   \n",
       "\n",
       "   rf_2_eng  log_eng  knn_eng  svc_eng  eng_xgb_1  eng_xgb_2  eng_xgb_3  \\\n",
       "0       1.0      0.0      0.0      0.0        0.0        0.0        0.0   \n",
       "1       1.0      0.0      0.0      0.0        0.0        0.0        0.0   \n",
       "2       1.0      0.0      0.0      0.0        0.0        0.0        0.0   \n",
       "3       1.0      0.0      0.0      0.0        0.0        0.0        0.0   \n",
       "4       1.0      0.0      0.0      0.0        0.0        0.0        0.0   \n",
       "\n",
       "   eng_gbm  \n",
       "0      0.0  \n",
       "1      1.0  \n",
       "2      0.0  \n",
       "3      0.0  \n",
       "4      1.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_meta = B_meta.astype(int)\n",
    "C_meta = C_meta.astype(int)\n",
    "test_meta = test_meta.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Imputer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thank you Will Koehrsen for an amazing kernel / this method!\n",
    "\n",
    "def model(features, labels , test_features, encoding = 'ohe', n_folds = 5):\n",
    "    \n",
    "    \"\"\"Train and test a light gradient boosting model using\n",
    "    cross validation. \n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "        features (pd.DataFrame): \n",
    "            dataframe of training features to use \n",
    "            for training a model. Must include the TARGET column.\n",
    "        test_features (pd.DataFrame): \n",
    "            dataframe of testing features to use\n",
    "            for making predictions with the model. \n",
    "        encoding (str, default = 'ohe'): \n",
    "            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n",
    "            n_folds (int, default = 5): number of folds to use for cross validation\n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "        submission (pd.DataFrame): \n",
    "            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n",
    "            predicted by the model.\n",
    "        feature_importances (pd.DataFrame): \n",
    "            dataframe with the feature importances from the model.\n",
    "        valid_metrics (pd.DataFrame): \n",
    "            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract the ids\n",
    "    #train_ids = features['SK_ID_CURR']\n",
    "    #test_ids = test_features['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    #labels = features['TARGET']\n",
    "    \n",
    "    # Remove the ids and target\n",
    "    #features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
    "    #features = features.drop(columns = ['TARGET'])\n",
    "    #test_features = test_features.drop(columns = ['SK_ID_CURR'])\n",
    "    \n",
    "    \n",
    "    # One Hot Encoding\n",
    "    if encoding == 'ohe':\n",
    "        features = pd.get_dummies(features)\n",
    "        test_features = pd.get_dummies(test_features)\n",
    "        \n",
    "        # Align the dataframes by the columns\n",
    "        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n",
    "        \n",
    "        # No categorical indices to record\n",
    "        cat_indices = 'auto'\n",
    "    \n",
    "    # Integer label encoding\n",
    "    elif encoding == 'le':\n",
    "        \n",
    "        # Create a label encoder\n",
    "        label_encoder = LabelEncoder()\n",
    "        \n",
    "        # List for storing categorical indices\n",
    "        cat_indices = []\n",
    "        \n",
    "        # Iterate through each column\n",
    "        for i, col in enumerate(features):\n",
    "            if features[col].dtype == 'object':\n",
    "                # Map the categorical features to integers\n",
    "                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n",
    "                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n",
    "\n",
    "                # Record the categorical indices\n",
    "                cat_indices.append(i)\n",
    "    \n",
    "    # Catch error if label encoding scheme is not valid\n",
    "    else:\n",
    "        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n",
    "        \n",
    "    print('Training Data Shape: ', features.shape)\n",
    "    print('Testing Data Shape: ', test_features.shape)\n",
    "    \n",
    "    # Extract feature names\n",
    "    feature_names = list(features.columns)\n",
    "    \n",
    "    # Convert to np arrays\n",
    "    features = np.array(features)\n",
    "    test_features = np.array(test_features)\n",
    "    \n",
    "    # Create the kfold object\n",
    "    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n",
    "    \n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "    \n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "    \n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "    \n",
    "    # Lists for recording validation and training scores\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "    \n",
    "    # Iterate through each fold\n",
    "    for train_indices, valid_indices in k_fold.split(features):\n",
    "        \n",
    "        # Training data for the fold\n",
    "        train_features, train_labels = features[train_indices], labels[train_indices]\n",
    "        # Validation data for the fold\n",
    "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
    "        \n",
    "        # Create the model\n",
    "        model = lgb.LGBMClassifier(n_estimators=1000, objective = 'binary', \n",
    "                                   is_unbalanced = True, learning_rate = 0.0001, \n",
    "                                   reg_alpha = 0.0, reg_lambda = 0.0, \n",
    "                                   subsample = 0.5, n_jobs = -1, random_state = 50)\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(train_features, train_labels, eval_metric = 'auc',\n",
    "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
    "                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n",
    "                  early_stopping_rounds = 100, verbose = 200)\n",
    "        \n",
    "        # Record the best iteration\n",
    "        best_iteration = model.best_iteration_\n",
    "        \n",
    "        # Record the feature importances\n",
    "        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
    "        \n",
    "        # Make predictions\n",
    "        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n",
    "        \n",
    "        # Record the out of fold predictions\n",
    "        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n",
    "        \n",
    "        # Record the best score\n",
    "        valid_score = model.best_score_['valid']['auc']\n",
    "        train_score = model.best_score_['train']['auc']\n",
    "        \n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.enable()\n",
    "        del model, train_features, valid_features\n",
    "        gc.collect()\n",
    "        \n",
    "    # Make the submission dataframe\n",
    "    #submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n",
    "    submission = pd.DataFrame({'TARGET': test_predictions})\n",
    "    \n",
    "    # Make the feature importance dataframe\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "    \n",
    "    # Overall validation score\n",
    "    valid_auc = roc_auc_score(labels, out_of_fold)\n",
    "    \n",
    "    # Add the overall scores to the metrics\n",
    "    valid_scores.append(valid_auc)\n",
    "    train_scores.append(np.mean(train_scores))\n",
    "    \n",
    "    # Needed for creating dataframe of validation scores\n",
    "    fold_names = list(range(n_folds))\n",
    "    fold_names.append('overall')\n",
    "    \n",
    "    # Dataframe of validation scores\n",
    "    metrics = pd.DataFrame({'fold': fold_names,\n",
    "                            'train': train_scores,\n",
    "                            'valid': valid_scores}) \n",
    "    \n",
    "    return submission, feature_importances, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_child_weight': 0.001, 'objective': None, 'reg_alpha': 0.0, 'subsample_for_bin': 200000, 'colsample_bytree': 1.0, 'random_state': None, 'max_depth': -1, 'subsample_freq': 0, 'silent': True, 'min_child_samples': 20, 'num_leaves': 31, 'n_jobs': -1, 'min_split_gain': 0.0, 'reg_lambda': 0.0, 'boosting_type': 'gbdt', 'learning_rate': 0.1, 'class_weight': None, 'subsample': 1.0, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "model = lgb.LGBMClassifier()\n",
    "print (model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = lgb.Dataset(data = B_meta, label = b_meta_labels.astype(int).reset_index(drop=True))\n",
    "test_set = lgb.Dataset(data = C_meta, label = c_meta_labels.astype(int).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(hyperparameters, iteration):\n",
    "    \"\"\"Objective function for grid and random search. Returns\n",
    "       the cross validation score from a set of hyperparameters.\"\"\"\n",
    "    \n",
    "    # Number of estimators will be found using early stopping\n",
    "    if 'n_estimators' in hyperparameters.keys():\n",
    "        del hyperparameters['n_estimators']\n",
    "    \n",
    "     # Perform n_folds cross validation\n",
    "    cv_results = lgb.cv(hyperparameters, train_set, num_boost_round = 10000, nfold = 5, \n",
    "                        early_stopping_rounds = 100, metrics = 'auc', seed = 42)\n",
    "    \n",
    "    # results to retun\n",
    "    score = cv_results['auc-mean'][-1]\n",
    "    estimators = len(cv_results['auc-mean'])\n",
    "    hyperparameters['n_estimators'] = estimators \n",
    "    \n",
    "    return [score, hyperparameters, iteration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing gridsearch with C_meta\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'objective' : ['binary'],\n",
    "    'boosting': ['gbdt', 'gbrt','goss', 'dart'],\n",
    "    'subsample_for_bin': list(range(50, 300000)),\n",
    "    'min_child_samples': list(range(10, 750)),\n",
    "    'reg_alpha': list(np.linspace(0, 1)),\n",
    "    'reg_lambda': list(np.linspace(0, 1)),\n",
    "    'colsample_bytree': list(np.linspace(0.6, 1, 10)),\n",
    "    'subsample': list(np.linspace(0.5, 1, 10)),\n",
    "    'is_unbalance': [True, False],\n",
    "    'max_bin' : list(range(255,100000)),\n",
    "    'learning_rate': list(np.logspace(np.log10(0.00000001), np.log10(0.5), base = 10, num = 1000)),\n",
    "    'num_leaves': list(range(2, 200000)),\n",
    "    'n_estimators' : list(range(1,2000000))\n",
    "}\n",
    "\n",
    "MAX_EVALS = 5\n",
    "\n",
    "import itertools\n",
    "\n",
    "def grid_search(param_grid, max_evals = MAX_EVALS):\n",
    "    \"\"\"Grid search algorithm (with limit on max evals)\"\"\"\n",
    "    \n",
    "    # Dataframe to store results\n",
    "    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
    "                              index = list(range(MAX_EVALS)))\n",
    "    \n",
    "    # https://codereview.stackexchange.com/questions/171173/list-all-possible-permutations-from-a-python-dictionary-of-lists\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    # Iterate through every possible combination of hyperparameters\n",
    "    for v in itertools.product(*values):\n",
    "        \n",
    "        # Create a hyperparameter dictionary\n",
    "        hyperparameters = dict(zip(keys, v))\n",
    "        \n",
    "        # Set the subsample ratio accounting for boosting type\n",
    "        hyperparameters['subsample'] = 1.0 if hyperparameters['boosting'] == 'goss' else hyperparameters['subsample']\n",
    "        \n",
    "        # Evalute the hyperparameters\n",
    "        eval_results = objective(hyperparameters, i)\n",
    "        \n",
    "        results.loc[i, :] = eval_results\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        # Normally would not limit iterations\n",
    "        if i > MAX_EVALS:\n",
    "            break\n",
    "       \n",
    "    # Sort with best score on top\n",
    "    results.sort_values('score', ascending = False, inplace = True)\n",
    "    results.reset_index(inplace = True)\n",
    "    \n",
    "    return results   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best validation score was 0.71399\n",
      "\n",
      "The best hyperparameters were:\n",
      "{'boosting': 'gbdt',\n",
      " 'colsample_bytree': 0.6,\n",
      " 'is_unbalance': True,\n",
      " 'learning_rate': 1e-08,\n",
      " 'max_bin': 255,\n",
      " 'metric': 'auc',\n",
      " 'min_child_samples': 10,\n",
      " 'n_estimators': 25,\n",
      " 'num_leaves': 2,\n",
      " 'objective': 'binary',\n",
      " 'reg_alpha': 0.0,\n",
      " 'reg_lambda': 0.0,\n",
      " 'subsample': 0.5,\n",
      " 'subsample_for_bin': 50,\n",
      " 'verbose': 1}\n"
     ]
    }
   ],
   "source": [
    "grid_results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
    "                              index = list(range(MAX_EVALS)))\n",
    "\n",
    "grid_results = grid_search(param_grid)\n",
    "\n",
    "print('The best validation score was {:.5f}'.format(grid_results.loc[0, 'score']))\n",
    "print('\\nThe best hyperparameters were:')\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(grid_results.loc[0, 'params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging with lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_bag(features, labels , test_features, random_seed, encoding = 'ohe', n_folds = 5):\n",
    "    \n",
    "    \"\"\"Train and test a light gradient boosting model using\n",
    "    cross validation. \n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "        features (pd.DataFrame): \n",
    "            dataframe of training features to use \n",
    "            for training a model. Must include the TARGET column.\n",
    "        test_features (pd.DataFrame): \n",
    "            dataframe of testing features to use\n",
    "            for making predictions with the model. \n",
    "        encoding (str, default = 'ohe'): \n",
    "            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n",
    "            n_folds (int, default = 5): number of folds to use for cross validation\n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "        submission (pd.DataFrame): \n",
    "            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n",
    "            predicted by the model.\n",
    "        feature_importances (pd.DataFrame): \n",
    "            dataframe with the feature importances from the model.\n",
    "        valid_metrics (pd.DataFrame): \n",
    "            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract the ids\n",
    "    #train_ids = features['SK_ID_CURR']\n",
    "    #test_ids = test_features['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    #labels = features['TARGET']\n",
    "    \n",
    "    # Remove the ids and target\n",
    "    #features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
    "    #features = features.drop(columns = ['TARGET'])\n",
    "    #test_features = test_features.drop(columns = ['SK_ID_CURR'])\n",
    "    \n",
    "    \n",
    "    # One Hot Encoding\n",
    "    if encoding == 'ohe':\n",
    "        features = pd.get_dummies(features)\n",
    "        test_features = pd.get_dummies(test_features)\n",
    "        \n",
    "        # Align the dataframes by the columns\n",
    "        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n",
    "        \n",
    "        # No categorical indices to record\n",
    "        cat_indices = 'auto'\n",
    "    \n",
    "    # Integer label encoding\n",
    "    elif encoding == 'le':\n",
    "        \n",
    "        # Create a label encoder\n",
    "        label_encoder = LabelEncoder()\n",
    "        \n",
    "        # List for storing categorical indices\n",
    "        cat_indices = []\n",
    "        \n",
    "        # Iterate through each column\n",
    "        for i, col in enumerate(features):\n",
    "            if features[col].dtype == 'object':\n",
    "                # Map the categorical features to integers\n",
    "                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n",
    "                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n",
    "\n",
    "                # Record the categorical indices\n",
    "                cat_indices.append(i)\n",
    "    \n",
    "    # Catch error if label encoding scheme is not valid\n",
    "    else:\n",
    "        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n",
    "        \n",
    "    print('Training Data Shape: ', features.shape)\n",
    "    print('Testing Data Shape: ', test_features.shape)\n",
    "    \n",
    "    # Extract feature names\n",
    "    feature_names = list(features.columns)\n",
    "    \n",
    "    # Convert to np arrays\n",
    "    features = np.array(features)\n",
    "    test_features = np.array(test_features)\n",
    "    \n",
    "    # Create the kfold object\n",
    "    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n",
    "    \n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "    \n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "    \n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "    \n",
    "    # Lists for recording validation and training scores\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "    \n",
    "    # Iterate through each fold\n",
    "    for train_indices, valid_indices in k_fold.split(features):\n",
    "        \n",
    "        # Training data for the fold\n",
    "        train_features, train_labels = features[train_indices], labels[train_indices]\n",
    "        # Validation data for the fold\n",
    "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
    "        \n",
    "        # Create the model\n",
    "        model = lgb.LGBMClassifier(n_estimators = 100, objective = 'binary', \n",
    "                                   is_unbalance = True, learning_rate = 1e-03, \n",
    "                                   reg_alpha = 0.0, reg_lambda = 0.0, max_bin = 255,\n",
    "                                   subsample = 0.5, n_jobs = -1, random_state = seed,\n",
    "                                   metric = 'auc', min_child_samples = 10, num_leaves = 10,\n",
    "                                   subsample_for_bin = 50, olsample_bytree = 0.6,)\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(train_features, train_labels, eval_metric = 'auc',\n",
    "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
    "                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n",
    "                  early_stopping_rounds = 100, verbose = 200)\n",
    "        \n",
    "        # Record the best iteration\n",
    "        best_iteration = model.best_iteration_\n",
    "        \n",
    "        # Record the feature importances\n",
    "        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
    "        \n",
    "        # Make predictions\n",
    "        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n",
    "        \n",
    "        # Record the out of fold predictions\n",
    "        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n",
    "        \n",
    "        # Record the best score\n",
    "        valid_score = model.best_score_['valid']['auc']\n",
    "        train_score = model.best_score_['train']['auc']\n",
    "        \n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.enable()\n",
    "        del model, train_features, valid_features\n",
    "        gc.collect()\n",
    "        \n",
    "    # Make the submission dataframe\n",
    "    #submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n",
    "    #submission = pd.DataFrame({'TARGET': test_predictions})\n",
    "    \n",
    "    # Make the feature importance dataframe\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "    \n",
    "    # Overall validation score\n",
    "    valid_auc = roc_auc_score(labels, out_of_fold)\n",
    "    \n",
    "    # Add the overall scores to the metrics\n",
    "    valid_scores.append(valid_auc)\n",
    "    train_scores.append(np.mean(train_scores))\n",
    "    \n",
    "    # Needed for creating dataframe of validation scores\n",
    "    fold_names = list(range(n_folds))\n",
    "    fold_names.append('overall')\n",
    "    \n",
    "    # Dataframe of validation scores\n",
    "    metrics = pd.DataFrame({'fold': fold_names,\n",
    "                            'train': train_scores,\n",
    "                            'valid': valid_scores}) \n",
    "    \n",
    "    return test_predictions #submission #, feature_importances, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape:  (184507, 21)\n",
      "Testing Data Shape:  (48744, 21)\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.721917\ttrain's auc: 0.718971\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.717717\ttrain's auc: 0.716788\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid's auc: 0.715062\ttrain's auc: 0.722375\n",
      "Early stopping, best iteration is:\n",
      "[116]\tvalid's auc: 0.715192\ttrain's auc: 0.722343\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.713506\ttrain's auc: 0.716685\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.721366\ttrain's auc: 0.71904\n",
      "predictions gathered from model.\n",
      "Training Data Shape:  (184507, 21)\n",
      "Testing Data Shape:  (48744, 21)\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.721917\ttrain's auc: 0.718971\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.717717\ttrain's auc: 0.716788\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid's auc: 0.715062\ttrain's auc: 0.722375\n",
      "Early stopping, best iteration is:\n",
      "[116]\tvalid's auc: 0.715192\ttrain's auc: 0.722343\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.713506\ttrain's auc: 0.716685\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.721366\ttrain's auc: 0.71904\n",
      "predictions gathered from model.\n",
      "Training Data Shape:  (184507, 21)\n",
      "Testing Data Shape:  (48744, 21)\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.721917\ttrain's auc: 0.718971\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.717717\ttrain's auc: 0.716788\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid's auc: 0.715062\ttrain's auc: 0.722375\n",
      "Early stopping, best iteration is:\n",
      "[116]\tvalid's auc: 0.715192\ttrain's auc: 0.722343\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.713506\ttrain's auc: 0.716685\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.721366\ttrain's auc: 0.71904\n",
      "predictions gathered from model.\n",
      "Training Data Shape:  (184507, 21)\n",
      "Testing Data Shape:  (48744, 21)\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.721917\ttrain's auc: 0.718971\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.717717\ttrain's auc: 0.716788\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid's auc: 0.715062\ttrain's auc: 0.722375\n",
      "Early stopping, best iteration is:\n",
      "[116]\tvalid's auc: 0.715192\ttrain's auc: 0.722343\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.713506\ttrain's auc: 0.716685\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.721366\ttrain's auc: 0.71904\n",
      "predictions gathered from model.\n",
      "Training Data Shape:  (184507, 21)\n",
      "Testing Data Shape:  (48744, 21)\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.721917\ttrain's auc: 0.718971\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.717717\ttrain's auc: 0.716788\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid's auc: 0.715062\ttrain's auc: 0.722375\n",
      "Early stopping, best iteration is:\n",
      "[116]\tvalid's auc: 0.715192\ttrain's auc: 0.722343\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.713506\ttrain's auc: 0.716685\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.721366\ttrain's auc: 0.71904\n",
      "predictions gathered from model.\n",
      "Training Data Shape:  (184507, 21)\n",
      "Testing Data Shape:  (48744, 21)\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.721917\ttrain's auc: 0.718971\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.717717\ttrain's auc: 0.716788\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid's auc: 0.715062\ttrain's auc: 0.722375\n",
      "Early stopping, best iteration is:\n",
      "[116]\tvalid's auc: 0.715192\ttrain's auc: 0.722343\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.713506\ttrain's auc: 0.716685\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.721366\ttrain's auc: 0.71904\n",
      "predictions gathered from model.\n",
      "Training Data Shape:  (184507, 21)\n",
      "Testing Data Shape:  (48744, 21)\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.721917\ttrain's auc: 0.718971\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.717717\ttrain's auc: 0.716788\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid's auc: 0.715062\ttrain's auc: 0.722375\n",
      "Early stopping, best iteration is:\n",
      "[116]\tvalid's auc: 0.715192\ttrain's auc: 0.722343\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.713506\ttrain's auc: 0.716685\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.721366\ttrain's auc: 0.71904\n",
      "predictions gathered from model.\n",
      "Training Data Shape:  (184507, 21)\n",
      "Testing Data Shape:  (48744, 21)\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.721917\ttrain's auc: 0.718971\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.717717\ttrain's auc: 0.716788\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid's auc: 0.715062\ttrain's auc: 0.722375\n",
      "Early stopping, best iteration is:\n",
      "[116]\tvalid's auc: 0.715192\ttrain's auc: 0.722343\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.713506\ttrain's auc: 0.716685\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.721366\ttrain's auc: 0.71904\n",
      "predictions gathered from model.\n",
      "Training Data Shape:  (184507, 21)\n",
      "Testing Data Shape:  (48744, 21)\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.721917\ttrain's auc: 0.718971\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.717717\ttrain's auc: 0.716788\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid's auc: 0.715062\ttrain's auc: 0.722375\n",
      "Early stopping, best iteration is:\n",
      "[116]\tvalid's auc: 0.715192\ttrain's auc: 0.722343\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.713506\ttrain's auc: 0.716685\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.721366\ttrain's auc: 0.71904\n",
      "predictions gathered from model.\n",
      "Training Data Shape:  (184507, 21)\n",
      "Testing Data Shape:  (48744, 21)\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.721917\ttrain's auc: 0.718971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.717717\ttrain's auc: 0.716788\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid's auc: 0.715062\ttrain's auc: 0.722375\n",
      "Early stopping, best iteration is:\n",
      "[116]\tvalid's auc: 0.715192\ttrain's auc: 0.722343\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.713506\ttrain's auc: 0.716685\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.721366\ttrain's auc: 0.71904\n",
      "predictions gathered from model.\n"
     ]
    }
   ],
   "source": [
    "bags = 10\n",
    "seed = 1\n",
    "bagged_predictions = np.zeros(test_meta.shape[0])\n",
    "bag_train_data = pd.concat([B_meta, C_meta], axis = 0)\n",
    "bag_train_labels = pd.concat([b_meta_labels.astype(int).reset_index(drop=True),\n",
    "                              c_meta_labels.astype(int).reset_index(drop=True)],\n",
    "                             axis = 0)\n",
    "for n in range (0, bags):\n",
    "    preds = model_bag(bag_train_data, bag_train_labels.reset_index(drop=True), test_meta, seed + n)\n",
    "    bagged_predictions+=preds\n",
    "    print('predictions gathered from model.')\n",
    "    \n",
    "bagged_predictions/=bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08642378, 0.09532946, 0.0864232 , ..., 0.0864232 , 0.0864232 ,\n",
       "       0.09532946])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagged_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TARGET\n",
       "0     0.0\n",
       "1     0.0\n",
       "2     0.0\n",
       "3     0.0\n",
       "4     0.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagged_predictions = convert_proba(bagged_predictions, 'TARGET')\n",
    "bagged_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = pd.read_csv('/Users/JoonH/credit_stacking.csv').drop('TARGET', axis = 1)\n",
    "submission = pd.concat([ids, bagged_predictions], axis = 1)\n",
    "submission.to_csv('/Users/JoonH/credit_bagged.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Predictions\n",
    "The model is tuned to fit the following hyperparameters, and are now ready to make the test predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thank you Will Koehrsen for an amazing kernel / this method!\n",
    "\n",
    "def model(features, labels , test_features, encoding = 'ohe', n_folds = 5):\n",
    "    \n",
    "    \"\"\"Train and test a light gradient boosting model using\n",
    "    cross validation. \n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "        features (pd.DataFrame): \n",
    "            dataframe of training features to use \n",
    "            for training a model. Must include the TARGET column.\n",
    "        test_features (pd.DataFrame): \n",
    "            dataframe of testing features to use\n",
    "            for making predictions with the model. \n",
    "        encoding (str, default = 'ohe'): \n",
    "            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n",
    "            n_folds (int, default = 5): number of folds to use for cross validation\n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "        submission (pd.DataFrame): \n",
    "            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n",
    "            predicted by the model.\n",
    "        feature_importances (pd.DataFrame): \n",
    "            dataframe with the feature importances from the model.\n",
    "        valid_metrics (pd.DataFrame): \n",
    "            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract the ids\n",
    "    #train_ids = features['SK_ID_CURR']\n",
    "    #test_ids = test_features['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    #labels = features['TARGET']\n",
    "    \n",
    "    # Remove the ids and target\n",
    "    #features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
    "    #features = features.drop(columns = ['TARGET'])\n",
    "    #test_features = test_features.drop(columns = ['SK_ID_CURR'])\n",
    "    \n",
    "    \n",
    "    # One Hot Encoding\n",
    "    if encoding == 'ohe':\n",
    "        features = pd.get_dummies(features)\n",
    "        test_features = pd.get_dummies(test_features)\n",
    "        \n",
    "        # Align the dataframes by the columns\n",
    "        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n",
    "        \n",
    "        # No categorical indices to record\n",
    "        cat_indices = 'auto'\n",
    "    \n",
    "    # Integer label encoding\n",
    "    elif encoding == 'le':\n",
    "        \n",
    "        # Create a label encoder\n",
    "        label_encoder = LabelEncoder()\n",
    "        \n",
    "        # List for storing categorical indices\n",
    "        cat_indices = []\n",
    "        \n",
    "        # Iterate through each column\n",
    "        for i, col in enumerate(features):\n",
    "            if features[col].dtype == 'object':\n",
    "                # Map the categorical features to integers\n",
    "                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n",
    "                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n",
    "\n",
    "                # Record the categorical indices\n",
    "                cat_indices.append(i)\n",
    "    \n",
    "    # Catch error if label encoding scheme is not valid\n",
    "    else:\n",
    "        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n",
    "        \n",
    "    print('Training Data Shape: ', features.shape)\n",
    "    print('Testing Data Shape: ', test_features.shape)\n",
    "    \n",
    "    # Extract feature names\n",
    "    feature_names = list(features.columns)\n",
    "    \n",
    "    # Convert to np arrays\n",
    "    features = np.array(features)\n",
    "    test_features = np.array(test_features)\n",
    "    \n",
    "    # Create the kfold object\n",
    "    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n",
    "    \n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "    \n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "    \n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "    \n",
    "    # Lists for recording validation and training scores\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "    \n",
    "    # Iterate through each fold\n",
    "    for train_indices, valid_indices in k_fold.split(features):\n",
    "        \n",
    "        # Training data for the fold\n",
    "        train_features, train_labels = features[train_indices], labels[train_indices]\n",
    "        # Validation data for the fold\n",
    "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
    "        \n",
    "        # Create the model\n",
    "        model = lgb.LGBMClassifier(n_estimators=1000, objective = 'binary', \n",
    "                                   is_unbalanced = True, learning_rate = 0.0001, \n",
    "                                   reg_alpha = 0.0, reg_lambda = 0.0, \n",
    "                                   subsample = 0.5, n_jobs = -1, random_state = 50)\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(train_features, train_labels, eval_metric = 'auc',\n",
    "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
    "                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n",
    "                  early_stopping_rounds = 100, verbose = 200)\n",
    "        \n",
    "        # Record the best iteration\n",
    "        best_iteration = model.best_iteration_\n",
    "        \n",
    "        # Record the feature importances\n",
    "        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
    "        \n",
    "        # Make predictions\n",
    "        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n",
    "        \n",
    "        # Record the out of fold predictions\n",
    "        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n",
    "        \n",
    "        # Record the best score\n",
    "        valid_score = model.best_score_['valid']['auc']\n",
    "        train_score = model.best_score_['train']['auc']\n",
    "        \n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.enable()\n",
    "        del model, train_features, valid_features\n",
    "        gc.collect()\n",
    "        \n",
    "    # Make the submission dataframe\n",
    "    #submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n",
    "    submission = pd.DataFrame({'TARGET': test_predictions})\n",
    "    \n",
    "    # Make the feature importance dataframe\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "    \n",
    "    # Overall validation score\n",
    "    valid_auc = roc_auc_score(labels, out_of_fold)\n",
    "    \n",
    "    # Add the overall scores to the metrics\n",
    "    valid_scores.append(valid_auc)\n",
    "    train_scores.append(np.mean(train_scores))\n",
    "    \n",
    "    # Needed for creating dataframe of validation scores\n",
    "    fold_names = list(range(n_folds))\n",
    "    fold_names.append('overall')\n",
    "    \n",
    "    # Dataframe of validation scores\n",
    "    metrics = pd.DataFrame({'fold': fold_names,\n",
    "                            'train': train_scores,\n",
    "                            'valid': valid_scores}) \n",
    "    \n",
    "    return submission, feature_importances, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape:  (184507, 21)\n",
      "Testing Data Shape:  (48744, 21)\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.721407\ttrain's auc: 0.721889\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[80]\tvalid's auc: 0.716875\ttrain's auc: 0.723575\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.72195\ttrain's auc: 0.720083\n",
      "Baseline metrics\n",
      "      fold     train     valid\n",
      "0        0  0.721889  0.721407\n",
      "1        1  0.723575  0.716875\n",
      "2        2  0.720083  0.721950\n",
      "3  overall  0.721849  0.604058\n"
     ]
    }
   ],
   "source": [
    "stacking_train = pd.concat([B_meta, C_meta], axis = 0)\n",
    "stacking_train_labels = pd.concat([b_meta_labels.astype(int).reset_index(drop=True),\n",
    "                              c_meta_labels.astype(int).reset_index(drop=True)],\n",
    "                             axis = 0)\n",
    "\n",
    "submission, fi, metrics = model(stacking_train,stacking_train_labels.reset_index(drop=True), test_meta)\n",
    "print('Baseline metrics')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "testy = pd.read_csv('/Users/JoonH/Desktop/Home_Credit_Challenge/all/application_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.concat([testy['SK_ID_CURR'], submission], axis = 1)\n",
    "\n",
    "submission.to_csv('credit_stacking.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Despite forming stacking and grid search to validate our approach, it appears to be that we haven't gone beyond our major score of 0.756 but even failed to reach that point! It is also possible that the meta model is experiencing some overfitting issues, as with more grid search with more options the model's public LB score began to drop to 0.68 from 0.69.\n",
    "\n",
    "It may be concluded that our model needs a more less-overfittable approach with use of other methods to further push its accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But wait... there are more ways, right? \n",
    "#### If this is all I could do, I should be #1 (But unfortunately I am not)\n",
    "\n",
    "Indeed, my approach was very amateurish and it may and definitely have undergone some assumptions that ignored many points to consider for a better, more accurate solution. Here are some of the following: \n",
    "\n",
    "If my data was time-sensitive, then we must make sure that our train data holds elements at a generation before our cross validation and test dataset. \n",
    "\n",
    "I used simple Out Of Fold validation, which is more prone to overfitting. We should try to use K-fold cross validation with the right value of K to prevent target leakage. \n",
    "\n",
    "We can have more various models in the early layers. \n",
    "\n",
    "We should also consider more feature engineering for our meta-models such as pairwise differences between meta features, row-wise statistics,etc. \n",
    "\n",
    "Last but not least, we should try to run StackNet and get it working for our solution, as it is one of the state-of-art method that will allow us to score much higher on the leaderboard. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
