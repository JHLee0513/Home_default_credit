{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering gives us many, many features, but do we REALLY need them all?\n",
    "\n",
    "With the specific feature matrix we got we get up to about 1000 features with one-hot encoding. However, too much features may lead to large amounts of noise and too many less important features. Thus with the help of a kernel we wish to build our final data set for our competition.\n",
    "\n",
    "We will be implementing a method called target permutation with help from https://www.kaggle.com/ogrellier/feature-selection-with-null-importances, a kernel which referrs to https://academic.oup.com/bioinformatics/article/26/10/1340/193348 for a more detailed explanation of hte procedure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Permutation\n",
    "\n",
    "Target Permutation is a feature selecting process where actual importance significance of features are tested against the distribution of feature importances when fitted to noise(generated by shuffling the target/response-variable regardless of its correspondence to the index of features).\n",
    "\n",
    "#### The process is as follows:\n",
    "\n",
    "1. Create null importances distributions\n",
    "\n",
    "2. Fit the model to the original target and gather feature importances\n",
    "\n",
    "3. Compute the probability of actual importance w.r.t. the null distribution.\n",
    "\n",
    "4. Compare actual importance to the mean and max of the null importances\n",
    "\n",
    "*note that the example in the kernel uses basic data but I will be using dataset engineered with feature tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', UserWarning)\n",
    "\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_spec = pd.read_csv('/Users/JoonH/total_feature_matrix_spec.csv')\n",
    "train = feature_spec[feature_spec['set'] == 'train']\n",
    "test = feature_spec[feature_spec['set'] == 'test']\n",
    "#train = pd.get_dummies(train)\n",
    "#test = pd.get_dummies(test)\n",
    "#train_labels= train['TARGET']\n",
    "#train, test = train.align(test, join = 'inner', axis = 1)\n",
    "#train['TARGET'] = train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking care of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JoonH\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "C:\\Users\\JoonH\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "categorical_feats = [\n",
    "    f for f in data.columns if data[f].dtype == 'object'\n",
    "]\n",
    "\n",
    "categorical_feats\n",
    "for f_ in categorical_feats:\n",
    "    data[f_], _ = pd.factorize(data[f_])\n",
    "    # Set feature type as categorical\n",
    "    data[f_] = data[f_].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importances(data, shuffle, seed=None):\n",
    "    # Gather real features\n",
    "    train_features = [f for f in data if f not in ['TARGET', 'SK_ID_CURR']]\n",
    "    # Go over fold and keep track of CV score (train and valid) and feature importances\n",
    "    \n",
    "    # Shuffle target if required\n",
    "    y = data['TARGET'].copy()\n",
    "    if shuffle:\n",
    "        # Here you could as well use a binomial distribution\n",
    "        y = data['TARGET'].copy().sample(frac=1.0)\n",
    "    \n",
    "    # Fit LightGBM in RF mode, yes it's quicker than sklearn RandomForest\n",
    "    dtrain = lgb.Dataset(data[train_features], y, free_raw_data=False, silent=True)\n",
    "    lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'rf',\n",
    "        'subsample': 0.623,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'num_leaves': 127,\n",
    "        'max_depth': 8,\n",
    "        'seed': seed,\n",
    "        'bagging_freq': 1,\n",
    "        'n_jobs': 4\n",
    "    }\n",
    "    \n",
    "    # Fit the model\n",
    "    clf = lgb.train(params=lgb_params, train_set=dtrain, num_boost_round=200, categorical_feature=categorical_feats)\n",
    "\n",
    "    # Get feature importances\n",
    "    imp_df = pd.DataFrame()\n",
    "    imp_df[\"feature\"] = list(train_features)\n",
    "    imp_df[\"importance_gain\"] = clf.feature_importance(importance_type='gain')\n",
    "    imp_df[\"importance_split\"] = clf.feature_importance(importance_type='split')\n",
    "    imp_df['trn_score'] = roc_auc_score(y, clf.predict(data[train_features]))\n",
    "    \n",
    "    return imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed the unexpected randomness of this world\n",
    "np.random.seed(123)\n",
    "# Get the actual importance, i.e. without shuffling\n",
    "actual_imp_df = get_feature_importances(data=data, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance_gain</th>\n",
       "      <th>importance_split</th>\n",
       "      <th>trn_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FLAG_DOCUMENT_15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.766461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LIVINGAPARTMENTS_MODE</td>\n",
       "      <td>97.386231</td>\n",
       "      <td>8</td>\n",
       "      <td>0.766461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EXT_SOURCE_1</td>\n",
       "      <td>48482.190393</td>\n",
       "      <td>1848</td>\n",
       "      <td>0.766461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LIVE_REGION_NOT_WORK_REGION</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.766461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FLAG_OWN_CAR</td>\n",
       "      <td>23.065200</td>\n",
       "      <td>2</td>\n",
       "      <td>0.766461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       feature  importance_gain  importance_split  trn_score\n",
       "0             FLAG_DOCUMENT_15         0.000000                 0   0.766461\n",
       "1        LIVINGAPARTMENTS_MODE        97.386231                 8   0.766461\n",
       "2                 EXT_SOURCE_1     48482.190393              1848   0.766461\n",
       "3  LIVE_REGION_NOT_WORK_REGION         0.000000                 0   0.766461\n",
       "4                 FLAG_OWN_CAR        23.065200                 2   0.766461"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_imp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with    2 of   10 (Spent   5.0 min)"
     ]
    }
   ],
   "source": [
    "# build null impotances distribution\n",
    "\n",
    "null_imp_df = pd.DataFrame()\n",
    "nb_runs = 10 #originally 80\n",
    "import time\n",
    "start = time.time()\n",
    "dsp = ''\n",
    "for i in range(nb_runs):\n",
    "    # Get current run importances\n",
    "    imp_df = get_feature_importances(data=data, shuffle=True)\n",
    "    imp_df['run'] = i + 1 \n",
    "    # Concat the latest importances with the old ones\n",
    "    null_imp_df = pd.concat([null_imp_df, imp_df], axis=0)\n",
    "    # Erase previous message\n",
    "    for l in range(len(dsp)):\n",
    "        print('\\b', end='', flush=True)\n",
    "    # Display current run and time used\n",
    "    spent = (time.time() - start) / 60\n",
    "    dsp = 'Done with %4d of %4d (Spent %5.1f min)' % (i + 1, nb_runs, spent)\n",
    "    print(dsp, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_imp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display example distributions\n",
    "\n",
    "def display_distributions(actual_imp_df_, null_imp_df_, feature_):\n",
    "    plt.figure(figsize=(13, 6))\n",
    "    gs = gridspec.GridSpec(1, 2)\n",
    "    # Plot Split importances\n",
    "    ax = plt.subplot(gs[0, 0])\n",
    "    a = ax.hist(null_imp_df_.loc[null_imp_df_['feature'] == feature_, 'importance_split'].values, label='Null importances')\n",
    "    ax.vlines(x=actual_imp_df_.loc[actual_imp_df_['feature'] == feature_, 'importance_split'].mean(), \n",
    "               ymin=0, ymax=np.max(a[0]), color='r',linewidth=10, label='Real Target')\n",
    "    ax.legend()\n",
    "    ax.set_title('Split Importance of %s' % feature_.upper(), fontweight='bold')\n",
    "    plt.xlabel('Null Importance (split) Distribution for %s ' % feature_.upper())\n",
    "    # Plot Gain importances\n",
    "    ax = plt.subplot(gs[0, 1])\n",
    "    a = ax.hist(null_imp_df_.loc[null_imp_df_['feature'] == feature_, 'importance_gain'].values, label='Null importances')\n",
    "    ax.vlines(x=actual_imp_df_.loc[actual_imp_df_['feature'] == feature_, 'importance_gain'].mean(), \n",
    "               ymin=0, ymax=np.max(a[0]), color='r',linewidth=10, label='Real Target')\n",
    "    ax.legend()\n",
    "    ax.set_title('Gain Importance of %s' % feature_.upper(), fontweight='bold')\n",
    "    plt.xlabel('Null Importance (gain) Distribution for %s ' % feature_.upper())\n",
    "    \n",
    "display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='LIVINGAPARTMENTS_AVG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization is key\n",
    "\n",
    "Instead of of looking at numbers and data, we can instead help our understanding by visualizing the feature importances and null importances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='CODE_GENDER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot the power of the exposed feature selection method is demonstrated. In particular it is well known that :\n",
    "1. Any feature sufficient variance can be used and made sense of by tree models. You can always find splits that help scoring better\n",
    "2. Correlated features have decaying importances once one of them is used by the model. The chosen feature will have strong importance and its correlated suite will have decaying importances\n",
    "3. The current method allows to :\n",
    "\n",
    "    a. Drop high variance features if they are not really related to the target\n",
    "    \n",
    "    b. Remove the decaying factor on correlated features, showing their real importance (or unbiased importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score features\n",
    "feature_scores = []\n",
    "for _f in actual_imp_df['feature'].unique():\n",
    "    f_null_imps_gain = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_gain'].values\n",
    "    f_act_imps_gain = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_gain'].mean()\n",
    "    gain_score = np.log(1e-10 + f_act_imps_gain / (1 + np.percentile(f_null_imps_gain, 75)))  # Avoid didvide by zero\n",
    "    f_null_imps_split = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_split'].values\n",
    "    f_act_imps_split = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_split'].mean()\n",
    "    split_score = np.log(1e-10 + f_act_imps_split / (1 + np.percentile(f_null_imps_split, 75)))  # Avoid didvide by zero\n",
    "    feature_scores.append((_f, split_score, gain_score))\n",
    "\n",
    "scores_df = pd.DataFrame(feature_scores, columns=['feature', 'split_score', 'gain_score'])\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "gs = gridspec.GridSpec(1, 2)\n",
    "# Plot Split importances\n",
    "ax = plt.subplot(gs[0, 0])\n",
    "sns.barplot(x='split_score', y='feature', data=scores_df.sort_values('split_score', ascending=False).iloc[0:70], ax=ax)\n",
    "ax.set_title('Feature scores wrt split importances', fontweight='bold', fontsize=14)\n",
    "# Plot Gain importances\n",
    "ax = plt.subplot(gs[0, 1])\n",
    "sns.barplot(x='gain_score', y='feature', data=scores_df.sort_values('gain_score', ascending=False).iloc[0:70], ax=ax)\n",
    "ax.set_title('Feature scores wrt gain importances', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data to csv\n",
    "null_imp_df.to_csv('null_importances_distribution_rf.csv')\n",
    "actual_imp_df.to_csv('actual_importances_ditribution_rf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score feature removal with different thresholds\n",
    "\n",
    "def score_feature_selection(df=None, train_features=None, cat_feats=None, target=None):\n",
    "    # Fit LightGBM \n",
    "    dtrain = lgb.Dataset(df[train_features], target, free_raw_data=False, silent=True)\n",
    "    lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': .1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'num_leaves': 31,\n",
    "        'max_depth': -1,\n",
    "        'seed': 13,\n",
    "        'n_jobs': 4,\n",
    "        'min_split_gain': .00001,\n",
    "        'reg_alpha': .00001,\n",
    "        'reg_lambda': .00001,\n",
    "        'metric': 'auc'\n",
    "    }\n",
    "    \n",
    "    # Fit the model\n",
    "    hist = lgb.cv(\n",
    "        params=lgb_params, \n",
    "        train_set=dtrain, \n",
    "        num_boost_round=2000,\n",
    "        categorical_feature=cat_feats,\n",
    "        nfold=5,\n",
    "        stratified=True,\n",
    "        shuffle=True,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=0,\n",
    "        seed=17\n",
    "    )\n",
    "    # Return the last mean / std values \n",
    "    return hist['auc-mean'][-1], hist['auc-stdv'][-1]\n",
    "\n",
    "# features = [f for f in data.columns if f not in ['SK_ID_CURR', 'TARGET']]\n",
    "# score_feature_selection(df=data[features], train_features=features, target=data['TARGET'])\n",
    "\n",
    "for threshold in [0, 10, 20, 30 , 40, 50 ,60 , 70, 80 , 90, 95, 99]:\n",
    "    split_feats = [_f for _f, _score, _ in correlation_scores if _score >= threshold]\n",
    "    split_cat_feats = [_f for _f, _score, _ in correlation_scores if (_score >= threshold) & (_f in categorical_feats)]\n",
    "    gain_feats = [_f for _f, _, _score in correlation_scores if _score >= threshold]\n",
    "    gain_cat_feats = [_f for _f, _, _score in correlation_scores if (_score >= threshold) & (_f in categorical_feats)]\n",
    "                                                                                             \n",
    "    print('Results for threshold %3d' % threshold)\n",
    "    split_results = score_feature_selection(df=data, train_features=split_feats, cat_feats=split_cat_feats, target=data['TARGET'])\n",
    "    print('\\t SPLIT : %.6f +/- %.6f' % (split_results[0], split_results[1]))\n",
    "    gain_results = score_feature_selection(df=data, train_features=gain_feats, cat_feats=gain_cat_feats, target=data['TARGET'])\n",
    "    print('\\t GAIN  : %.6f +/- %.6f' % (gain_results[0], gain_results[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Unfortunately, due to our lack of sufficient RAM for the task, it is quite difficult for us to try the entire kernel. Here we can see that we may test all our features and analyze whether we *really* need them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
