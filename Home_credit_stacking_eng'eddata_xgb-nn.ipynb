{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from featuretools.primitives import AggregationPrimitive, make_agg_primitive\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import featuretools as ft\n",
    "from sklearn.preprocessing import MinMaxScaler, Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"\\\\Users\\\\JoonH\\\\total_feature_matrix_spec.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[data['set'] == 'train']\n",
    "test = data[data['set'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.get_dummies(train)\n",
    "test = pd.get_dummies(test)\n",
    "train_labels= train['TARGET']\n",
    "train, test = train.align(test, join = 'inner', axis = 1)\n",
    "train['TARGET'] = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.drop('TARGET', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devide train data into 3: train A, B, and C\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_A, train_B = train_test_split(train, test_size = 0.6, random_state = 3)\n",
    "train_B, train_C = train_test_split(train_B, test_size = 0.3, random_state = 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(123004, 1157) (129154, 1157) (55353, 1157) (48744, 1156)\n"
     ]
    }
   ],
   "source": [
    "print(train_A.shape, train_B.shape, train_C.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    \n",
    "    if 'TARGET' in data.columns:\n",
    "        data_features = data.drop('TARGET', axis = 1)\n",
    "    else:\n",
    "        data_features = data.copy()\n",
    "        \n",
    "    #encoded_data_features = pd.get_dummies(data_features)\n",
    "    imputer = Imputer(strategy = 'median')\n",
    "    filled_data_features = imputer.fit_transform(data_features)\n",
    "    scaler = MinMaxScaler(feature_range = (0, 10))\n",
    "    scaled_data_features = scaler.fit_transform(filled_data_features)\n",
    "    \n",
    "    return scaled_data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First layer models: 3 xgboost, 1 lightgbm, 2 neural nets\n",
    "import xgboost as xgb\n",
    "\n",
    "def train_xgb (clf, params, features, num_round): # features should include target variable in them\n",
    "    train_labels = features['TARGET']\n",
    "    features = process_data(features)\n",
    "    dtrain = clf.DMatrix(features, label = train_labels)\n",
    "    bst = clf.train(params, dtrain, num_round)\n",
    "    \n",
    "    return bst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# short tree\n",
    "param_1 = {'max_depth' : 10, 'eta': 0.001, 'silent': 1, 'objective': 'binary:logistic'}\n",
    "param_1['nthread'] = 4\n",
    "param_1['eval_metric'] = 'auc'\n",
    "param_1['eval_metric']= ['auc', 'ams@0']\n",
    "\n",
    "bst_1 = train_xgb(xgb, param_1, train_A, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medium tree\n",
    "param_2 = {'max_depth' : 100, 'eta': 0.0001, 'silent': 1, 'objective': 'binary:logistic'}\n",
    "param_2['nthread'] = 4\n",
    "param_2['eval_metric'] = 'auc'\n",
    "param_2['eval_metric']= ['auc', 'ams@0']\n",
    "\n",
    "bst_2 = train_xgb(xgb, param_2, train_A, 1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high(deep) tree\n",
    "param_3 = {'max_depth' : 750, 'eta': 0.0001, 'silent': 1, 'objective': 'binary:logistic'}\n",
    "param_3['nthread'] = 4\n",
    "param_3['eval_metric'] = 'auc'\n",
    "param_3['eval_metric']= ['auc', 'ams@0']\n",
    "\n",
    "bst_3 = train_xgb(xgb, param_3, train_A, 850)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_first_layer(xgb, clf, dataB, dataC, test):\n",
    "    print('Preparing data...')\n",
    "    B_features = process_data(dataB)\n",
    "    C_features = process_data(dataC)\n",
    "    test_features = process_data(test)\n",
    "    db = xgb.DMatrix(B_features)\n",
    "    dc = xgb.DMatrix(C_features)\n",
    "    dtest = xgb.DMatrix(test_features)\n",
    "    print('Data prepared, predicting...')\n",
    "    B_meta = clf.predict(db)\n",
    "    C_meta = clf.predict(dc)\n",
    "    test_meta = clf.predict(dtest)\n",
    "    print('Done!')\n",
    "    \n",
    "    return B_meta, C_meta, test_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Data prepared, predicting...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "bst_1_B_meta, bst_1_C_meta, bst_1_test_meta = predict_first_layer(xgb, bst_1, train_B, \n",
    "                                                            train_C, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Data prepared, predicting...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "bst_2_B_meta, bst_2_C_meta, bst_2_test_meta = predict_first_layer(xgb, bst_2, train_B, \n",
    "                                                            train_C, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Data prepared, predicting...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "bst_3_B_meta, bst_3_C_meta, bst_3_test_meta = predict_first_layer(xgb, bst_3, train_B, \n",
    "                                                            train_C, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_1_B_meta = pd.DataFrame(bst_1_B_meta, columns = ['eng_xgb_1'])\n",
    "bst_2_B_meta = pd.DataFrame(bst_2_B_meta, columns = ['eng_xgb_2'])\n",
    "bst_3_B_meta = pd.DataFrame(bst_3_B_meta, columns = ['eng_xgb_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_1_C_meta = pd.DataFrame(bst_1_C_meta, columns = ['eng_xgb_1'])\n",
    "bst_2_C_meta = pd.DataFrame(bst_2_C_meta, columns = ['eng_xgb_2'])\n",
    "bst_3_C_meta = pd.DataFrame(bst_3_C_meta, columns = ['eng_xgb_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_1_test_meta = pd.DataFrame(bst_1_test_meta, columns = ['eng_xgb_1'])\n",
    "bst_2_test_meta = pd.DataFrame(bst_2_test_meta, columns = ['eng_xgb_2'])\n",
    "bst_3_test_meta = pd.DataFrame(bst_3_test_meta, columns = ['eng_xgb_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_meta_data = pd.concat([bst_1_B_meta, bst_2_B_meta, bst_3_B_meta],axis = 1)\n",
    "C_meta_data = pd.concat([bst_1_C_meta, bst_2_C_meta, bst_3_C_meta],axis = 1)\n",
    "test_meta_data = pd.concat([bst_1_test_meta, bst_2_test_meta, bst_3_test_meta],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving xgboost results to csv\n",
    "B_meta_data.to_csv('eng_data_B_meta_xgb')\n",
    "C_meta_data.to_csv('eng_data_C_meta_xgb')\n",
    "test_meta_data.to_csv('eng_data_test_meta_xgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add Neural network --- work on this point after logging out as local machine will most likely crash due to memory overload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run up to line 7 for basic import and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
