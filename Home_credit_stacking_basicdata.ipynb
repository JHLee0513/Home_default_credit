{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"\\\\Users\\\\JoonH\\\\Desktop\\\\Home_Credit_Challenge\\\\all\\\\application_train.csv\")\n",
    "test = pd.read_csv(\"\\\\Users\\\\JoonH\\\\Desktop\\\\Home_Credit_Challenge\\\\all\\\\application_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.get_dummies(train)\n",
    "test = pd.get_dummies(test)\n",
    "train_labels= train['TARGET']\n",
    "train, test = train.align(test, join = 'inner', axis = 1)\n",
    "train['TARGET'] = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = test['SK_ID_CURR']\n",
    "train = train.drop('SK_ID_CURR', axis = 1)\n",
    "test = test.drop('SK_ID_CURR', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devide train data into 3: train A, B, and C\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_A, train_B = train_test_split(train, test_size = 0.6, random_state = 3)\n",
    "train_B, train_C = train_test_split(train_B, test_size = 0.5, random_state = 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(123004, 242) (92253, 242) (92254, 242) (48744, 241)\n"
     ]
    }
   ],
   "source": [
    "print(train_A.shape, train_B.shape, train_C.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First level models to use\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier(n_estimators = 100, learning_rate = 0.01) # AdaBoost\n",
    "bag = BaggingClassifier(n_estimators = 50, max_samples = 0.2,\n",
    "                        max_features = 1.0, warm_start = True,\n",
    "                        n_jobs = -1) # Bagging\n",
    "gbc = GradientBoostingClassifier(learning_rate = 0.01,\n",
    "                                 n_estimators = 600,\n",
    "                                 max_depth = 10,\n",
    "                                 subsample = 0.5) # Gradient Boosting Classifier\n",
    "rf = RandomForestClassifier(n_estimators = 100,\n",
    "                            n_jobs = -1)# Random Forest\n",
    "gp = GaussianProcessClassifier(n_jobs = -1) # Gaussian Process\n",
    "log = LogisticRegression(C = 0.1, n_jobs = -1) # Logistic Regression\n",
    "rid = RidgeClassifier(alpha = 0.5) # Ridge Classifier\n",
    "sgd = SGDClassifier(n_jobs = -1) # SGD Classifier\n",
    "gnb = GaussianNB() # Gaussian Naive Bayes\n",
    "knn = KNeighborsClassifier(n_neighbors = 2, n_jobs = -1) # K Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_first_layer_model(clf, train_data):\n",
    "    train_labels = train_data['TARGET']\n",
    "    train_features = train_data.drop('TARGET', axis = 1)\n",
    "    #encoded_train_features = pd.get_dummies(train_features)\n",
    "\n",
    "    imputer = Imputer(strategy = 'median')\n",
    "    filled_train_features = imputer.fit_transform(train_features)\n",
    "    scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "    scaled_train_features = scaler.fit_transform(filled_train_features)\n",
    "    \n",
    "    clf.fit(scaled_train_features, train_labels)\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all the first level models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = train_first_layer_model(ada, train_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag = train_first_layer_model(bag, train_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = train_first_layer_model(gbc, train_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = train_first_layer_model(rf, train_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JoonH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    }
   ],
   "source": [
    "log = train_first_layer_model(log, train_A) #log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = train_first_layer_model(rid, train_A) #rid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JoonH\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "sgd = train_first_layer_model(sgd, train_A) #sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = train_first_layer_model(gnb, train_A) #gnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = train_first_layer_model(knn, train_A) #knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions of train_B, train_C, and test with first layer models\n",
    "# build a meta model and train it with B, and validate with C for hypter parameter tuning\n",
    "# train meta model on A,B,C and predict test\n",
    "\n",
    "def process_data(data):\n",
    "    \n",
    "    if 'TARGET' in data.columns:\n",
    "        data_features = data.drop('TARGET', axis = 1)\n",
    "    else:\n",
    "        data_features = data.copy()\n",
    "        \n",
    "    #encoded_data_features = pd.get_dummies(data_features)\n",
    "    imputer = Imputer(strategy = 'median')\n",
    "    filled_data_features = imputer.fit_transform(data_features)\n",
    "    scaler = MinMaxScaler(feature_range = (0, 10))\n",
    "    scaled_data_features = scaler.fit_transform(filled_data_features)\n",
    "    \n",
    "    return scaled_data_features\n",
    "\n",
    "def predict_first_layer(clf, dataB, dataC, test):\n",
    "    B_features = process_data(dataB)\n",
    "    C_features = process_data(dataC)\n",
    "    print(B_features.shape, C_features.shape)\n",
    "    test_features = process_data(test)\n",
    "    B_meta = clf.predict(B_features)\n",
    "    C_meta = clf.predict(C_features)\n",
    "    test_meta = clf.predict(test_features)\n",
    "    \n",
    "    return B_meta, C_meta, test_meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92253, 241) (92254, 241)\n"
     ]
    }
   ],
   "source": [
    "ada_B_meta, ada_C_meta, ada_test_meta = predict_first_layer(ada, train_B, \n",
    "                                                            train_C, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92253, 241) (92254, 241)\n"
     ]
    }
   ],
   "source": [
    "bag_B_meta, bag_C_meta, bag_test_meta = predict_first_layer(bag, train_B, \n",
    "                                                            train_C, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "#et_B_meta, et_C_meta, et_test_meta = predict_first_layer(et, train_B, \n",
    "#                                                            train_C, test)\n",
    "# keeps generating model error during prediction, left untouched for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92253, 241) (92254, 241)\n"
     ]
    }
   ],
   "source": [
    "gbc_B_meta, gbc_C_meta, gbc_test_meta = predict_first_layer(gbc, train_B, \n",
    "                                                            train_C, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92253, 241) (92254, 241)\n"
     ]
    }
   ],
   "source": [
    "rf_B_meta, rf_C_meta, rf_test_meta = predict_first_layer(rf, train_B, \n",
    "                                                            train_C, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92253, 241) (92254, 241)\n"
     ]
    }
   ],
   "source": [
    "log_B_meta, log_C_meta, log_test_meta = predict_first_layer(log, train_B, \n",
    "                                                            train_C, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92253, 241) (92254, 241)\n",
      "(92253, 241) (92254, 241)\n",
      "(92253, 241) (92254, 241)\n",
      "(92253, 241) (92254, 241)\n"
     ]
    }
   ],
   "source": [
    "rid_B_meta, rid_C_meta, rid_test_meta = predict_first_layer(rid, train_B, \n",
    "                                                            train_C, test)\n",
    "sgd_B_meta, sgd_C_meta, sgd_test_meta = predict_first_layer(sgd, train_B, \n",
    "                                                            train_C, test)\n",
    "gnb_B_meta, gnb_C_meta, gnb_test_meta = predict_first_layer(gnb, train_B, \n",
    "                                                            train_C, test)\n",
    "knn_B_meta, knn_C_meta, knn_test_meta = predict_first_layer(knn, train_B, \n",
    "                                                            train_C, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build meta_models, fit them to train B, validate on train C\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create metadata for meta_B\n",
    "\n",
    "ada_B_meta = pd.DataFrame(ada_B_meta, columns = ['ada'])\n",
    "bag_B_meta = pd.DataFrame(bag_B_meta, columns = ['bag'])\n",
    "gbc_B_meta = pd.DataFrame(gbc_B_meta, columns = ['gbc'])\n",
    "rf_B_meta = pd.DataFrame(rf_B_meta, columns = ['rf'])\n",
    "log_B_meta = pd.DataFrame(log_B_meta, columns = ['log'])\n",
    "rid_B_meta = pd.DataFrame(rid_B_meta, columns = ['rid'])\n",
    "sgd_B_meta = pd.DataFrame(sgd_B_meta, columns = ['sgd'])\n",
    "gnb_B_meta = pd.DataFrame(gnb_B_meta, columns = ['gnb'])\n",
    "knn_B_meta = pd.DataFrame(knn_B_meta, columns = ['knn'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_C_meta = pd.DataFrame(ada_C_meta, columns = ['ada'])\n",
    "bag_C_meta = pd.DataFrame(bag_C_meta, columns = ['bag'])\n",
    "gbc_C_meta = pd.DataFrame(gbc_C_meta, columns = ['gbc'])\n",
    "rf_C_meta = pd.DataFrame(rf_C_meta, columns = ['rf'])\n",
    "log_C_meta = pd.DataFrame(log_C_meta, columns = ['log'])\n",
    "rid_C_meta = pd.DataFrame(rid_C_meta, columns = ['rid'])\n",
    "sgd_C_meta = pd.DataFrame(sgd_C_meta, columns = ['sgd'])\n",
    "gnb_C_meta = pd.DataFrame(gnb_C_meta, columns = ['gnb'])\n",
    "knn_C_meta = pd.DataFrame(knn_C_meta, columns = ['knn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_test_meta = pd.DataFrame(ada_test_meta, columns = ['ada'])\n",
    "bag_test_meta = pd.DataFrame(bag_test_meta, columns = ['bag'])\n",
    "gbc_test_meta = pd.DataFrame(gbc_test_meta, columns = ['gbc'])\n",
    "rf_test_meta = pd.DataFrame(rf_test_meta, columns = ['rf'])\n",
    "log_test_meta = pd.DataFrame(log_test_meta, columns = ['log'])\n",
    "rid_test_meta = pd.DataFrame(rid_test_meta, columns = ['rid'])\n",
    "sgd_test_meta = pd.DataFrame(sgd_test_meta, columns = ['sgd'])\n",
    "gnb_test_meta = pd.DataFrame(gnb_test_meta, columns = ['gnb'])\n",
    "knn_test_meta = pd.DataFrame(knn_test_meta, columns = ['knn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "B_meta_data = pd.concat([ada_B_meta, bag_B_meta, gbc_B_meta, rf_B_meta,\n",
    "                         log_B_meta, rid_B_meta, sgd_B_meta,\n",
    "                         gnb_B_meta, knn_B_meta],axis = 1)\n",
    "\n",
    "C_meta_data = pd.concat([ada_C_meta, bag_C_meta, gbc_C_meta, rf_C_meta,\n",
    "                         log_C_meta, rid_C_meta, sgd_C_meta,\n",
    "                         gnb_C_meta, knn_C_meta],axis = 1)\n",
    "\n",
    "test_meta_data = pd.concat([ada_test_meta, bag_test_meta, gbc_test_meta, \n",
    "                            rf_test_meta, log_test_meta, rid_test_meta,\n",
    "                            sgd_test_meta, gnb_test_meta, knn_test_meta],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.concat([B_meta_data.copy(), labels],axis = 1)\n",
    "test_features = test_meta_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape:  (92253, 9)\n",
      "Testing Data Shape:  (48744, 9)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'[    0     1     2 ... 92249 92250 92252] not in index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-388-b908b976422e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;31m# Training data for the fold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mtrain_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[1;31m# Validation data for the fold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mvalid_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalid_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalid_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2131\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2132\u001b[0m             \u001b[1;31m# either boolean or fancy integer index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2133\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2134\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2135\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2175\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2176\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2177\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2178\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[0;32m   1267\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1268\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[1;32m-> 1269\u001b[1;33m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[0;32m   1270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '[    0     1     2 ... 92249 92250 92252] not in index'"
     ]
    }
   ],
   "source": [
    "features = pd.get_dummies(features)\n",
    "test_features = pd.get_dummies(test_features)\n",
    "        \n",
    "# Align the dataframes by the columns\n",
    "features, test_features = features.align(test_features, join = 'inner', axis = 1)\n",
    "        \n",
    "# No categorical indices to record\n",
    "cat_indices = 'auto'\n",
    "      \n",
    "print('Training Data Shape: ', features.shape)\n",
    "print('Testing Data Shape: ', test_features.shape)\n",
    "    \n",
    "# Extract feature names\n",
    "feature_names = list(features.columns)\n",
    "    \n",
    "# Convert to np arrays\n",
    "features = np.array(features)\n",
    "test_features = np.array(test_features)\n",
    "    \n",
    "# Create the kfold object\n",
    "k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n",
    "    \n",
    "# Empty array for feature importances\n",
    "feature_importance_values = np.zeros(len(feature_names))\n",
    "    \n",
    "# Empty array for test predictions\n",
    "test_predictions = np.zeros(test_features.shape[0])\n",
    "    \n",
    "# Empty array for out of fold validation predictions\n",
    "out_of_fold = np.zeros(features.shape[0])\n",
    "    \n",
    "# Lists for recording validation and training scores\n",
    "valid_scores = []\n",
    "train_scores = []\n",
    "    \n",
    "# Iterate through each fold\n",
    "for train_indices, valid_indices in k_fold.split(features):\n",
    "        \n",
    "    # Training data for the fold\n",
    "    train_features, train_labels = features[train_indices], labels[train_indices]\n",
    "    # Validation data for the fold\n",
    "    valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
    "        \n",
    "    # Create the model\n",
    "    model = lgb.LGBMClassifier(n_estimators=1000, objective = 'binary', \n",
    "                                   class_weight = 'balanced', learning_rate = 0.001, \n",
    "                                   reg_alpha = 0.1, reg_lambda = 0.1, \n",
    "                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n",
    "        \n",
    "    # Train the model\n",
    "    model.fit(train_features, train_labels, eval_metric = 'auc',\n",
    "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
    "                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n",
    "                  early_stopping_rounds = 100, verbose = 200)\n",
    "        \n",
    "    # Record the best iteration\n",
    "    best_iteration = model.best_iteration_\n",
    "        \n",
    "    # Record the feature importances\n",
    "    feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
    "        \n",
    "    # Make predictions\n",
    "    test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n",
    "        \n",
    "    # Record the out of fold predictions\n",
    "    out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n",
    "        \n",
    "    # Record the best score\n",
    "    valid_score = model.best_score_['valid']['auc']\n",
    "    train_score = model.best_score_['train']['auc']\n",
    "        \n",
    "    valid_scores.append(valid_score)\n",
    "    train_scores.append(train_score)\n",
    "        \n",
    "    # Clean up memory\n",
    "    gc.enable()\n",
    "    del model, train_features, valid_features\n",
    "    gc.collect()\n",
    "        \n",
    "# Make the submission dataframe\n",
    "submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n",
    "    \n",
    "# Make the feature importance dataframe\n",
    "feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "    \n",
    "# Overall validation score\n",
    "valid_auc = roc_auc_score(labels, out_of_fold)\n",
    "    \n",
    "# Add the overall scores to the metrics\n",
    "valid_scores.append(valid_auc)\n",
    "train_scores.append(np.mean(train_scores))\n",
    "    \n",
    "# Needed for creating dataframe of validation scores\n",
    "fold_names = list(range(n_folds))\n",
    "fold_names.append('overall')\n",
    "    \n",
    "# Dataframe of validation scores\n",
    "metrics = pd.DataFrame({'fold': fold_names,\n",
    "                            'train': train_scores,\n",
    "                            'valid': valid_scores}) \n",
    "    \n",
    "#return submission, feature_importances, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thank you Will Koehrsen for an amazing kernel / this method!\n",
    "\n",
    "def model(features, test_features, encoding = 'ohe', n_folds = 5):\n",
    "    \n",
    "    \"\"\"Train and test a light gradient boosting model using\n",
    "    cross validation. \n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "        features (pd.DataFrame): \n",
    "            dataframe of training features to use \n",
    "            for training a model. Must include the TARGET column.\n",
    "        test_features (pd.DataFrame): \n",
    "            dataframe of testing features to use\n",
    "            for making predictions with the model. \n",
    "        encoding (str, default = 'ohe'): \n",
    "            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n",
    "            n_folds (int, default = 5): number of folds to use for cross validation\n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "        submission (pd.DataFrame): \n",
    "            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n",
    "            predicted by the model.\n",
    "        feature_importances (pd.DataFrame): \n",
    "            dataframe with the feature importances from the model.\n",
    "        valid_metrics (pd.DataFrame): \n",
    "            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract the ids\n",
    "    #train_ids = features['SK_ID_CURR']\n",
    "    #test_ids = test_features['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    labels = features['TARGET']\n",
    "    \n",
    "    # Remove the ids and target\n",
    "    #features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
    "    features = features.drop(columns = ['TARGET'])\n",
    "    #test_features = test_features.drop(columns = ['SK_ID_CURR'])\n",
    "    \n",
    "    \n",
    "    # One Hot Encoding\n",
    "    if encoding == 'ohe':\n",
    "        features = pd.get_dummies(features)\n",
    "        test_features = pd.get_dummies(test_features)\n",
    "        \n",
    "        # Align the dataframes by the columns\n",
    "        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n",
    "        \n",
    "        # No categorical indices to record\n",
    "        cat_indices = 'auto'\n",
    "    \n",
    "    # Integer label encoding\n",
    "    elif encoding == 'le':\n",
    "        \n",
    "        # Create a label encoder\n",
    "        label_encoder = LabelEncoder()\n",
    "        \n",
    "        # List for storing categorical indices\n",
    "        cat_indices = []\n",
    "        \n",
    "        # Iterate through each column\n",
    "        for i, col in enumerate(features):\n",
    "            if features[col].dtype == 'object':\n",
    "                # Map the categorical features to integers\n",
    "                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n",
    "                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n",
    "\n",
    "                # Record the categorical indices\n",
    "                cat_indices.append(i)\n",
    "    \n",
    "    # Catch error if label encoding scheme is not valid\n",
    "    else:\n",
    "        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n",
    "        \n",
    "    print('Training Data Shape: ', features.shape)\n",
    "    print('Testing Data Shape: ', test_features.shape)\n",
    "    \n",
    "    # Extract feature names\n",
    "    feature_names = list(features.columns)\n",
    "    \n",
    "    # Convert to np arrays\n",
    "    features = np.array(features)\n",
    "    test_features = np.array(test_features)\n",
    "    \n",
    "    # Create the kfold object\n",
    "    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n",
    "    \n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "    \n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "    \n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "    \n",
    "    # Lists for recording validation and training scores\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "    \n",
    "    # Iterate through each fold\n",
    "    for train_indices, valid_indices in k_fold.split(features):\n",
    "        \n",
    "        # Training data for the fold\n",
    "        train_features, train_labels = features[train_indices], labels[train_indices]\n",
    "        # Validation data for the fold\n",
    "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
    "        \n",
    "        # Create the model\n",
    "        model = lgb.LGBMClassifier(n_estimators=1000, objective = 'binary', \n",
    "                                   class_weight = 'balanced', learning_rate = 0.01, \n",
    "                                   reg_alpha = 0.1, reg_lambda = 0.1, \n",
    "                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(train_features, train_labels, eval_metric = 'auc',\n",
    "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
    "                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n",
    "                  early_stopping_rounds = 100, verbose = 200)\n",
    "        \n",
    "        # Record the best iteration\n",
    "        best_iteration = model.best_iteration_\n",
    "        \n",
    "        # Record the feature importances\n",
    "        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
    "        \n",
    "        # Make predictions\n",
    "        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n",
    "        \n",
    "        # Record the out of fold predictions\n",
    "        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n",
    "        \n",
    "        # Record the best score\n",
    "        valid_score = model.best_score_['valid']['auc']\n",
    "        train_score = model.best_score_['train']['auc']\n",
    "        \n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.enable()\n",
    "        del model, train_features, valid_features\n",
    "        gc.collect()\n",
    "        \n",
    "    # Make the submission dataframe\n",
    "    #submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n",
    "    submission = pd.DataFrame({'TARGET': test_predictions})\n",
    "    \n",
    "    # Make the feature importance dataframe\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "    \n",
    "    # Overall validation score\n",
    "    valid_auc = roc_auc_score(labels, out_of_fold)\n",
    "    \n",
    "    # Add the overall scores to the metrics\n",
    "    valid_scores.append(valid_auc)\n",
    "    train_scores.append(np.mean(train_scores))\n",
    "    \n",
    "    # Needed for creating dataframe of validation scores\n",
    "    fold_names = list(range(n_folds))\n",
    "    fold_names.append('overall')\n",
    "    \n",
    "    # Dataframe of validation scores\n",
    "    metrics = pd.DataFrame({'fold': fold_names,\n",
    "                            'train': train_scores,\n",
    "                            'valid': valid_scores}) \n",
    "    \n",
    "    return submission, feature_importances, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape:  (92253, 9)\n",
      "Testing Data Shape:  (48744, 9)\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[67]\tvalid's auc: 0.526473\ttrain's auc: 0.520415\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[98]\tvalid's auc: 0.51951\ttrain's auc: 0.522189\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.522222\ttrain's auc: 0.521511\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.51921\ttrain's auc: 0.522252\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[68]\tvalid's auc: 0.520697\ttrain's auc: 0.521894\n",
      "Baseline metrics\n",
      "      fold     train     valid\n",
      "0        0  0.520415  0.526473\n",
      "1        1  0.522189  0.519510\n",
      "2        2  0.521511  0.522222\n",
      "3        3  0.522252  0.519210\n",
      "4        4  0.521894  0.520697\n",
      "5  overall  0.521652  0.521194\n"
     ]
    }
   ],
   "source": [
    "submission, fi, metrics = model(features, test_features)\n",
    "print('Baseline metrics')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('credit_meta_baseline.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metamodel - log\n",
    "log_meta = LogisticRegression()\n",
    "log_meta.fit(B_meta, B_labels)\n",
    "#find validation score of baseline meta classifier with C_meta\n",
    "\n",
    "#gridsearch, randomsearch on log_meta with C_meta, C labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit meta models to A,B,C combined to train data again, predict on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plan:\n",
    "##### 1. Break down training into A,B,C\n",
    "##### 2. Build numerous various models (fitted to A) and form predictions for B,C.test\n",
    "##### 3. Build a meta_model (lightgbm) that trains on B_meta and B_true_label\n",
    "##### 4. Validate model on C_meta and C_true_label\n",
    "##### 5. Make predictions on test with meta_model via test_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
